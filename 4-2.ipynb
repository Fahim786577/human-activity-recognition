{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,Input,InputLayer,Reshape,GRU,LSTM,Concatenate\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import models,Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Conv2DTranspose, UpSampling2D,GlobalAveragePooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, threshold=0.8):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs[\"accuracy\"]>self.threshold:\n",
    "            self.model.save_weights('G:/Project Server Pred/model_weights/weights-threshold{}.h5'.format(self.threshold), overwrite=False)\n",
    "            self.threshold =0.9\n",
    "            print(\"80 percent saved\")\n",
    "        if logs[\"accuracy\"]>self.threshold:\n",
    "            self.model.save_weights('G:/Project Server Pred/model_weights/weights-threshold{}.h5'.format(self.threshold), overwrite=False)\n",
    "            self.threshold = 1\n",
    "            print(\"90 percent saved\")\n",
    "            \n",
    "def callbacks(name, tensorboard = False,customCallback=False, patience=35,location='G:/Project Server Pred/model_weights/'):\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(location+'/weights-{}.h5'.format(str(name)), monitor='val_loss', save_best_only=True, save_weights_only=True),\n",
    "        EarlyStopping(patience=patience, monitor='val_loss', min_delta=0, mode='min'),\n",
    "        \n",
    "    ]\n",
    "    if tensorboard:\n",
    "        callbacks.append(TensorBoard(log_dir='./logs/{}'.format(name), histogram_freq=0, write_graph=True, write_images=True))\n",
    "    if customCallback:\n",
    "        callbacks.append(CustomCallback())\n",
    "    return callbacks\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def normalize(x):\n",
    "    x_reshaped = x.reshape(-1,x.shape[-1])\n",
    "    mean,std=x_reshaped.mean(axis=0),x_reshaped.std(axis=0)\n",
    "    x_reshaped = (x_reshaped - mean)/std\n",
    "    print(mean,std)\n",
    "    return x_reshaped.reshape(x.shape)    #,mean,std\n",
    "\n",
    "def reshape_Data(x,channel_first=True):   \n",
    "    \n",
    "    if channel_first:\n",
    "        return x.reshape((x.shape[0],1,x.shape[1],x.shape[2]))\n",
    "    else:\n",
    "        return x.reshape((x.shape[0],x.shape[1],x.shape[2],1))\n",
    "    \n",
    "def reshapeBack_Data(x,channel_first=True):\n",
    "    \n",
    "    if channel_first:\n",
    "        return x.reshape((x.shape[0],x.shape[2],x.shape[3]))\n",
    "    else:\n",
    "        return x.reshape((x.shape[0],x.shape[1],x.shape[2]))\n",
    "    \n",
    "def my_PCA(data,gravity_axis=1):\n",
    "    data_n = data.copy()\n",
    "    for i,d in enumerate(data_n):\n",
    "        pca = PCA()\n",
    "        d = np.delete(d[:,:3],gravity_axis,axis=1)\n",
    "        pca.fit(d)\n",
    "        data_t = pca.transform(d)\n",
    "        \n",
    "        if gravity_axis==1:\n",
    "            data_n[i][:,0] = data_t[:,0]\n",
    "            data_n[i][:,2] = data_t[:,1]\n",
    "        else:\n",
    "            print(\"need to add code\")\n",
    "    return data_n\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128, 9)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128, 64)           10752     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 128, 64, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 62, 64, 64)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 64, 32)        10272     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 64, 32)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 7, 64, 32)         128       \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 224, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 224, 64)           24832     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 224, 64)           0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 224, 64, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 110, 64, 64)       384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 55, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 26, 64, 32)        10272     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 297       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 57,449\n",
      "Trainable params: 57,321\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x2944fdead60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LSTM_CNN_SANDWICH(input_shape,output_shape):\n",
    "    K.set_image_data_format('channels_last')\n",
    "    n_outputs = output_shape\n",
    "    input_shape = input_shape\n",
    "    \n",
    "    input           = Input(shape=input_shape)\n",
    "    lstm_1          = Bidirectional(LSTM(32,return_sequences=True))(input)\n",
    "    activation_1    = Activation(\"relu\")(lstm_1)\n",
    "    reshape_layer_1 = Reshape((activation_1.shape[1],activation_1.shape[2],1))(activation_1)\n",
    "    cnn_1           = Conv2D(64, (5,1), strides=(2,1), activation='relu')(reshape_layer_1)\n",
    "    max_pool_1      = MaxPooling2D((2,1), strides=(2,1))(cnn_1)\n",
    "    cnn_1_2         = Conv2D(32, (5,1), strides=(2,1), activation='relu')(max_pool_1)\n",
    "    max_pool_1_2    = MaxPooling2D((2,1), strides=(2,1))(cnn_1_2)\n",
    "    batch_norm_1    = BatchNormalization()(max_pool_1_2)\n",
    "\n",
    "    reshape_layer_2 = Reshape((-1,cnn_1.shape[3]))(batch_norm_1)\n",
    "    lstm_2          = Bidirectional(LSTM(32,return_sequences=True))(reshape_layer_2)\n",
    "    activation_2    = Activation(\"relu\")(lstm_2)\n",
    "    reshape_layer_3 = Reshape((activation_2.shape[1],activation_2.shape[2],1))(activation_2)\n",
    "    cnn_2           = Conv2D(64, (5,1), strides=(2,1), activation='relu')(reshape_layer_3)\n",
    "    max_pool_2      = MaxPooling2D((2,1), strides=(2,1))(cnn_2)\n",
    "    cnn_2_2         = Conv2D(32, (5,1), strides=(2,1), activation='relu')(max_pool_2)\n",
    "    max_pool_2_2    = MaxPooling2D((2,1), strides=(2,1))(cnn_2_2)\n",
    "    batch_norm_2    = BatchNormalization()(max_pool_2_2)\n",
    "\n",
    "    global_avg      = GlobalAveragePooling2D()(batch_norm_2)\n",
    "\n",
    "    dense           = Dense(n_outputs)(global_avg)\n",
    "    activation_3    = Activation(\"softmax\")(dense)\n",
    "\n",
    "    model = Model(inputs = input, outputs= activation_3)\n",
    "    model.summary()\n",
    "\n",
    "    rmsprop = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy',f1_m])\n",
    "\n",
    "    return model\n",
    "    \n",
    "LSTM_CNN_SANDWICH((128,9),9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128, 9)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 128, 64)           10752     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128, 64)           24832     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 128, 64, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 62, 30, 64)        1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 31, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 29, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 112,265\n",
      "Trainable params: 112,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x29457cc3700>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LSTM_CNN(input_shape=None,n_outputs=None):\n",
    "    \n",
    "    K.set_image_data_format('channels_last')\n",
    "    n_outputs = n_outputs\n",
    "    input_shape = input_shape\n",
    "    input           = Input(shape=input_shape)\n",
    "\n",
    "    lstm_1          = Bidirectional(LSTM(32,return_sequences=True))(input)\n",
    "    activation_1    = Activation(\"relu\")(lstm_1)\n",
    "    lstm_2          = Bidirectional(LSTM(32,return_sequences=True))(activation_1)\n",
    "    activation_2    = Activation(\"relu\")(lstm_2)\n",
    "\n",
    "    reshape_layer_1 = Reshape((lstm_2.shape[1],lstm_2.shape[2],1))(activation_2)\n",
    "    \n",
    "    cnn_1           = Conv2D(64, (5,5), strides=(2,2), activation='relu')(reshape_layer_1)\n",
    "    max_pool_1      = MaxPooling2D((2,2), strides=(2,2))(cnn_1)\n",
    "    cnn_2           = Conv2D(128, (3,3), strides=(1,1), activation='relu')(max_pool_1)\n",
    "    global_avg      = GlobalAveragePooling2D()(cnn_2)\n",
    "    dense           = Dense(n_outputs,activation='softmax',kernel_regularizer=keras.regularizers.l2(0.005))(global_avg)\n",
    "\n",
    "    model = Model(inputs = input, outputs= dense)\n",
    "    model.summary()\n",
    "\n",
    "    rmsprop = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy',f1_m])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "LSTM_CNN((128,9),9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 128, 9)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 128, 64)      10752       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 128, 9, 1)    0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 64)      0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 62, 3, 64)    1664        reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 128, 64)      24832       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 60, 1, 128)   73856       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, 64)      0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 128)          0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 128, 64, 1)   0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 128, 1)       0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 62, 30, 64)   1664        reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 128, 64)      8704        reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 31, 15, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, 64)      0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 29, 13, 128)  73856       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 128, 64)      24832       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 128)          0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 64)      0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8320)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 9)            74889       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 295,049\n",
      "Trainable params: 295,049\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x294e98183d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ConvLSTM_PARALLEL(input_shape=None,n_outputs=None):\n",
    "    \n",
    "    K.set_image_data_format('channels_last')\n",
    "    input_shape = input_shape\n",
    "    input           = Input(shape=input_shape)\n",
    "\n",
    "    lstm_1          = Bidirectional(LSTM(32,return_sequences=True,activation='relu'))(input)\n",
    "    activation_1    = Activation(\"relu\")(lstm_1)\n",
    "    lstm_2          = Bidirectional(LSTM(32,return_sequences=True,activation ='relu'))(activation_1)\n",
    "    activation_2    = Activation(\"relu\")(lstm_2)\n",
    "    reshape_layer_1 = Reshape((lstm_2.shape[1],lstm_2.shape[2],1))(activation_2)\n",
    "    \n",
    "    cnn_1           = Conv2D(64, (5,5), strides=(2,2), activation='relu')(reshape_layer_1)\n",
    "    max_pool_1      = MaxPooling2D((2,2), strides=(2,2))(cnn_1)\n",
    "    cnn_2           = Conv2D(128, (3,3), strides=(1,1), activation='relu')(max_pool_1)\n",
    "    global_avg      = GlobalAveragePooling2D()(cnn_2)\n",
    "    flatten         = Flatten()(global_avg)\n",
    "\n",
    "    reshape_2         = Reshape((input.shape[1],input.shape[2],1))(input)\n",
    "    cnn_1_2           = Conv2D(64, (5,5), strides=(2,2), activation='relu')(reshape_2)\n",
    "    #max_pool_1_2     = MaxPooling2D((2,2), strides=(2,2))(cnn_1_2)\n",
    "    cnn_2_2           = Conv2D(128, (3,3), strides=(1,1), activation='relu')(cnn_1_2)\n",
    "    global_avg_2      = GlobalAveragePooling2D()(cnn_2_2)\n",
    "\n",
    "    reshape_layer_1_2 = Reshape((global_avg_2.shape[-1],1))(global_avg_2)\n",
    "    lstm_1_2          = Bidirectional(LSTM(32,return_sequences=True,activation='relu'))(reshape_layer_1_2)\n",
    "    activation_1_2    = Activation(\"relu\")(lstm_1_2)\n",
    "    lstm_2_2          = Bidirectional(LSTM(32,return_sequences=True,activation ='relu'))(activation_1_2)\n",
    "    activation_2_2    = Activation(\"relu\")(lstm_2_2)\n",
    "    flatten_2         = Flatten()(activation_2_2)\n",
    "\n",
    "    concatenate       = Concatenate()([flatten,flatten_2])\n",
    "    dense             = Dense(n_outputs,activation='softmax',kernel_regularizer=keras.regularizers.l2(0.005))(concatenate)\n",
    "\n",
    "\n",
    "\n",
    "    model = Model(inputs = input, outputs= dense)\n",
    "    model.summary()\n",
    "\n",
    "    rmsprop = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy',f1_m])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "ConvLSTM_PARALLEL((128,9),9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 128, 9)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 128, 64)      10752       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 128, 9, 1)    0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 128, 64)      0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 62, 3, 64)    1664        reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 128, 64)      24832       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 60, 1, 128)   73856       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 64)      0           bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 128)          0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 128)          0           global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8320)         0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 9)            74889       concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 185,993\n",
      "Trainable params: 185,993\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x294e9708940>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LSTM_CNN_PARALLEL(input_shape=None,n_outputs=None):\n",
    "    \n",
    "    K.set_image_data_format('channels_last')\n",
    "    input_shape = input_shape\n",
    "    input           = Input(shape=input_shape)\n",
    "\n",
    "    lstm_1          = Bidirectional(LSTM(32,return_sequences=True,activation='relu'))(input)\n",
    "    activation_1    = Activation(\"relu\")(lstm_1)\n",
    "    lstm_2          = Bidirectional(LSTM(32,return_sequences=True,activation ='relu'))(activation_1)\n",
    "    activation_2    = Activation(\"relu\")(lstm_2)\n",
    "    flatten         = Flatten()(activation_2)\n",
    "\n",
    "    reshape_2         = Reshape((input.shape[1],input.shape[2],1))(input)\n",
    "    cnn_1_2           = Conv2D(64, (5,5), strides=(2,2), activation='relu')(reshape_2)\n",
    "    #max_pool_1_2     = MaxPooling2D((2,2), strides=(2,2))(cnn_1_2)\n",
    "    cnn_2_2           = Conv2D(128, (3,3), strides=(1,1), activation='relu')(cnn_1_2)\n",
    "    global_avg_2      = GlobalAveragePooling2D()(cnn_2_2)\n",
    "\n",
    "    flatten_2         = Flatten()(global_avg_2)\n",
    "\n",
    "    concatenate       = Concatenate()([flatten,flatten_2])\n",
    "    dense             = Dense(n_outputs,activation='softmax',kernel_regularizer=keras.regularizers.l2(0.005))(concatenate)\n",
    "\n",
    "\n",
    "\n",
    "    model = Model(inputs = input, outputs= dense)\n",
    "    model.summary()\n",
    "\n",
    "    rmsprop = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy',f1_m])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "LSTM_CNN_PARALLEL((128,9),9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 9)]          0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 128, 32)           4128      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128, 32)           6336      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 128, 32, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 62, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 7, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 29, 5, 128)        73856     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 87,657\n",
      "Trainable params: 87,401\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x1acb65bbca0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GRU_CNN(input_shape,output_shape):\n",
    "    K.set_image_data_format('channels_last')\n",
    "    \n",
    "    n_outputs = output_shape\n",
    "    input_shape = input_shape\n",
    "    \n",
    "    input = Input(shape = input_shape)\n",
    "    gru_1 = GRU(32, return_sequences = True, activation = 'relu')(input)\n",
    "    gru_2 = GRU(32, return_sequences = True, activation = 'relu')(gru_1)\n",
    "    rehsape_layer_1 = Reshape((gru_2.shape[1],gru_2.shape[2],1))(gru_2)\n",
    "    cnn_1 = Conv2D(64, (5,5), strides=(2,2), activation='relu')(rehsape_layer_1)\n",
    "    max_pool_1 = MaxPooling2D((2,2), strides=(2,2)) (cnn_1)\n",
    "    cnn_2 = Conv2D(128, (3,3), strides=(1,1), activation='relu')(max_pool_1)\n",
    "\n",
    "    global_avg = GlobalAveragePooling2D()(cnn_2)\n",
    "    batch_norm = BatchNormalization()(global_avg)\n",
    "\n",
    "    dense = Dense(n_outputs,activation ='softmax') (batch_norm)\n",
    "\n",
    "    model = Model(inputs = input,outputs = dense)\n",
    "    model.summary()\n",
    "    rmsprop=keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy',f1_m])\n",
    "\n",
    "    return model\n",
    "\n",
    "GRU_CNN((128,9),9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 128, 9)]          0         \n",
      "_________________________________________________________________\n",
      "reshape_13 (Reshape)         (None, 128, 9, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 124, 9, 128)       768       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 62, 9, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 60, 9, 64)         24640     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 30, 9, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 28, 8, 32)         12320     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 14, 8, 32)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 14, 8, 32)         128       \n",
      "_________________________________________________________________\n",
      "reshape_14 (Reshape)         (None, 112, 32)           0         \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 112, 32)           6336      \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 32)                6336      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 50,953\n",
      "Trainable params: 50,825\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x1acdf534760>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CNN_GRU(input_shape,output_shape):\n",
    "    K.set_image_data_format('channels_last')\n",
    "    \n",
    "    n_outputs = output_shape\n",
    "    input_shape = input_shape\n",
    "    \n",
    "    input   = Input(shape=input_shape)\n",
    "    reshape_layer = Reshape((input.shape[1],input.shape[2],1)) (input)\n",
    "    cnn_1         = Conv2D(128, (5,1), strides=(1,1), activation='relu') (reshape_layer)\n",
    "    max_pool_1    = MaxPooling2D((2,1)) (cnn_1)\n",
    "    cnn_2         = Conv2D(64, (3,1), strides=(1,1), activation='relu') (max_pool_1)\n",
    "    max_pool_2    = MaxPooling2D((2,1)) (cnn_2)\n",
    "    cnn_3         = Conv2D(32, (3,2), strides=(1,1), activation='relu') (max_pool_2)\n",
    "    max_pool_3    = MaxPooling2D((2,1)) (cnn_3)\n",
    "    batch_norm    = BatchNormalization()(max_pool_3)\n",
    "\n",
    "    reshape_layer_2 = Reshape((-1,batch_norm.shape[3]))(batch_norm)\n",
    "    gru_1 = GRU(32, return_sequences = True, activation = 'relu')(reshape_layer_2)\n",
    "    gru_2 = GRU(32, activation = 'relu')(gru_1)\n",
    "\n",
    "    #global_avg = GlobalAveragePooling2D()(gru_2)\n",
    "    batch_norm = BatchNormalization()(gru_2)\n",
    "\n",
    "    dense = Dense(n_outputs,activation ='softmax') (batch_norm)\n",
    "\n",
    "    model = Model(inputs = input,outputs = dense)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    rmsprop=keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy',f1_m])\n",
    "    \n",
    "    return model\n",
    "\n",
    "CNN_GRU((128,9),9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.73906856e-04 -6.18831527e-05  1.69143052e-04  7.68531343e-03\n",
      " -3.65057419e-03 -5.69706057e-03  8.10025421e-01  1.91012355e-02\n",
      "  7.07049269e-02] [0.1955623  0.12778783 0.10316915 0.44456255 0.41230313 0.30430312\n",
      " 0.40220988 0.41860727 0.3401073 ]\n",
      "[ 5.73906856e-04 -6.18831527e-05  1.69143052e-04  7.68531343e-03\n",
      " -3.65057419e-03 -5.69706057e-03  8.10025421e-01  1.91012355e-02\n",
      "  7.07049269e-02] [0.1955623  0.12778783 0.10316915 0.44456255 0.41230313 0.30430312\n",
      " 0.40220988 0.41860727 0.3401073 ]\n",
      "(8618, 128, 9) (8618, 7) (1356, 128, 9) (1356, 7)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def normalize(x,mean = None,std = None):\n",
    "    x_reshaped = x.reshape(-1,x.shape[-1])\n",
    "    if mean is None or std is None:\n",
    "        mean,std   = x_reshaped.mean(axis=0),x_reshaped.std(axis=0)\n",
    "    x_reshaped = (x_reshaped - mean)/std\n",
    "    print(mean,std)\n",
    "    return x_reshaped.reshape(x.shape),mean,std    #,mean,std\n",
    "\n",
    "location = \"G:/Project Server Pred/Dataset/\"\n",
    "\n",
    "\n",
    "\n",
    "trainX = pickle.load(open(location+\"UCI_ALL_trainX_7act\", \"rb\"))\n",
    "trainY = pickle.load(open(location+\"UCI_ALL_trainY_7act\", \"rb\"))\n",
    "\n",
    "valX = pickle.load(open(location+\"UCI_ALL_valX_7act\", \"rb\"))\n",
    "valY = pickle.load(open(location+\"UCI_ALL_valY_7act\", \"rb\"))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "X_train,m,s = normalize(trainX)\n",
    "X_val,_,_   = normalize(valX,m,s)\n",
    "Y_train     = to_categorical(trainY-1)\n",
    "Y_val       = to_categorical(valY-1)\n",
    "\n",
    "index = np.arange(len(X_train))\n",
    "np.random.shuffle(index)\n",
    "X_train = X_train[index]\n",
    "Y_train = Y_train[index]\n",
    "\n",
    "print(X_train.shape,Y_train.shape,X_val.shape,Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 128, 9)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_46 (Bidirectio (None, 128, 64)           10752     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "reshape_36 (Reshape)         (None, 128, 64, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 62, 64, 64)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 31, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 14, 64, 32)        10272     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 7, 64, 32)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 64, 32)         128       \n",
      "_________________________________________________________________\n",
      "reshape_37 (Reshape)         (None, 224, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_47 (Bidirectio (None, 224, 64)           24832     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 224, 64)           0         \n",
      "_________________________________________________________________\n",
      "reshape_38 (Reshape)         (None, 224, 64, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 110, 64, 64)       384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 55, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 26, 64, 32)        10272     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 13, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 13, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_23  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 12)                396       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 57,548\n",
      "Trainable params: 57,420\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "270/270 - 31s - loss: 1.8342 - accuracy: 0.4653 - f1_m: 0.0952 - val_loss: 2.2398 - val_accuracy: 0.2920 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/150\n",
      "270/270 - 18s - loss: 0.9823 - accuracy: 0.7651 - f1_m: 0.4963 - val_loss: 0.9496 - val_accuracy: 0.7883 - val_f1_m: 0.4211\n",
      "Epoch 3/150\n",
      "270/270 - 18s - loss: 0.5633 - accuracy: 0.8516 - f1_m: 0.8501 - val_loss: 0.5305 - val_accuracy: 0.8496 - val_f1_m: 0.8235\n",
      "Epoch 4/150\n",
      "270/270 - 18s - loss: 0.4050 - accuracy: 0.8811 - f1_m: 0.8902 - val_loss: 0.4779 - val_accuracy: 0.8400 - val_f1_m: 0.8154\n",
      "Epoch 5/150\n",
      "270/270 - 18s - loss: 0.3315 - accuracy: 0.8960 - f1_m: 0.9018 - val_loss: 0.3951 - val_accuracy: 0.8599 - val_f1_m: 0.8362\n",
      "Epoch 6/150\n",
      "270/270 - 18s - loss: 0.2867 - accuracy: 0.9097 - f1_m: 0.9086 - val_loss: 0.3631 - val_accuracy: 0.8665 - val_f1_m: 0.8515\n",
      "Epoch 7/150\n",
      "270/270 - 18s - loss: 0.2539 - accuracy: 0.9178 - f1_m: 0.9157 - val_loss: 0.2870 - val_accuracy: 0.8864 - val_f1_m: 0.8874\n",
      "Epoch 8/150\n",
      "270/270 - 17s - loss: 0.2298 - accuracy: 0.9249 - f1_m: 0.9179 - val_loss: 0.3429 - val_accuracy: 0.8673 - val_f1_m: 0.8474\n",
      "Epoch 9/150\n",
      "270/270 - 17s - loss: 0.2127 - accuracy: 0.9324 - f1_m: 0.9208 - val_loss: 0.2810 - val_accuracy: 0.8953 - val_f1_m: 0.8842\n",
      "Epoch 10/150\n",
      "270/270 - 17s - loss: 0.2011 - accuracy: 0.9322 - f1_m: 0.9241 - val_loss: 0.3284 - val_accuracy: 0.8614 - val_f1_m: 0.8531\n",
      "Epoch 11/150\n",
      "270/270 - 17s - loss: 0.1868 - accuracy: 0.9373 - f1_m: 0.9284 - val_loss: 0.3150 - val_accuracy: 0.8695 - val_f1_m: 0.8621\n",
      "Epoch 12/150\n",
      "270/270 - 17s - loss: 0.1771 - accuracy: 0.9422 - f1_m: 0.9354 - val_loss: 0.2345 - val_accuracy: 0.9063 - val_f1_m: 0.9040\n",
      "Epoch 13/150\n",
      "270/270 - 17s - loss: 0.1684 - accuracy: 0.9417 - f1_m: 0.9372 - val_loss: 0.2759 - val_accuracy: 0.8857 - val_f1_m: 0.8839\n",
      "Epoch 14/150\n",
      "270/270 - 17s - loss: 0.1593 - accuracy: 0.9433 - f1_m: 0.9407 - val_loss: 0.2266 - val_accuracy: 0.9086 - val_f1_m: 0.9106\n",
      "Epoch 15/150\n",
      "270/270 - 17s - loss: 0.1511 - accuracy: 0.9480 - f1_m: 0.9451 - val_loss: 0.2802 - val_accuracy: 0.8879 - val_f1_m: 0.8871\n",
      "Epoch 16/150\n",
      "270/270 - 17s - loss: 0.1453 - accuracy: 0.9487 - f1_m: 0.9462 - val_loss: 0.2821 - val_accuracy: 0.8945 - val_f1_m: 0.8952\n",
      "Epoch 17/150\n",
      "270/270 - 17s - loss: 0.1400 - accuracy: 0.9524 - f1_m: 0.9504 - val_loss: 0.2807 - val_accuracy: 0.9012 - val_f1_m: 0.8964\n",
      "Epoch 18/150\n",
      "270/270 - 17s - loss: 0.1367 - accuracy: 0.9513 - f1_m: 0.9503 - val_loss: 0.2777 - val_accuracy: 0.8909 - val_f1_m: 0.8912\n",
      "Epoch 19/150\n",
      "270/270 - 17s - loss: 0.1291 - accuracy: 0.9540 - f1_m: 0.9531 - val_loss: 0.2829 - val_accuracy: 0.8968 - val_f1_m: 0.8914\n",
      "Epoch 20/150\n",
      "270/270 - 17s - loss: 0.1258 - accuracy: 0.9547 - f1_m: 0.9540 - val_loss: 0.2853 - val_accuracy: 0.9004 - val_f1_m: 0.8946\n",
      "Epoch 21/150\n",
      "270/270 - 17s - loss: 0.1209 - accuracy: 0.9574 - f1_m: 0.9565 - val_loss: 0.3278 - val_accuracy: 0.8805 - val_f1_m: 0.8809\n",
      "Epoch 22/150\n",
      "270/270 - 17s - loss: 0.1173 - accuracy: 0.9571 - f1_m: 0.9563 - val_loss: 0.3111 - val_accuracy: 0.8864 - val_f1_m: 0.8890\n",
      "Epoch 23/150\n",
      "270/270 - 17s - loss: 0.1114 - accuracy: 0.9599 - f1_m: 0.9595 - val_loss: 0.3629 - val_accuracy: 0.8658 - val_f1_m: 0.8679\n",
      "Epoch 24/150\n",
      "270/270 - 17s - loss: 0.1072 - accuracy: 0.9605 - f1_m: 0.9596 - val_loss: 0.3359 - val_accuracy: 0.8857 - val_f1_m: 0.8863\n",
      "Epoch 25/150\n",
      "270/270 - 17s - loss: 0.1071 - accuracy: 0.9603 - f1_m: 0.9597 - val_loss: 0.3021 - val_accuracy: 0.8938 - val_f1_m: 0.8950\n",
      "Epoch 26/150\n",
      "270/270 - 17s - loss: 0.1012 - accuracy: 0.9638 - f1_m: 0.9632 - val_loss: 0.3021 - val_accuracy: 0.8997 - val_f1_m: 0.9008\n",
      "Epoch 27/150\n",
      "270/270 - 17s - loss: 0.0981 - accuracy: 0.9647 - f1_m: 0.9640 - val_loss: 0.3590 - val_accuracy: 0.8850 - val_f1_m: 0.8824\n",
      "Epoch 28/150\n",
      "270/270 - 17s - loss: 0.0965 - accuracy: 0.9643 - f1_m: 0.9641 - val_loss: 0.3493 - val_accuracy: 0.8916 - val_f1_m: 0.8925\n",
      "Epoch 29/150\n",
      "270/270 - 17s - loss: 0.0947 - accuracy: 0.9657 - f1_m: 0.9649 - val_loss: 0.3793 - val_accuracy: 0.8776 - val_f1_m: 0.8807\n",
      "Epoch 30/150\n",
      "270/270 - 17s - loss: 0.0910 - accuracy: 0.9677 - f1_m: 0.9665 - val_loss: 0.3609 - val_accuracy: 0.8842 - val_f1_m: 0.8881\n",
      "Epoch 31/150\n",
      "270/270 - 17s - loss: 0.0879 - accuracy: 0.9686 - f1_m: 0.9676 - val_loss: 0.3552 - val_accuracy: 0.8938 - val_f1_m: 0.8978\n",
      "Epoch 32/150\n",
      "270/270 - 17s - loss: 0.0864 - accuracy: 0.9690 - f1_m: 0.9691 - val_loss: 0.3579 - val_accuracy: 0.8842 - val_f1_m: 0.8880\n",
      "Epoch 33/150\n",
      "270/270 - 17s - loss: 0.0815 - accuracy: 0.9712 - f1_m: 0.9712 - val_loss: 0.3913 - val_accuracy: 0.8732 - val_f1_m: 0.8793\n",
      "Epoch 34/150\n",
      "270/270 - 17s - loss: 0.0796 - accuracy: 0.9730 - f1_m: 0.9723 - val_loss: 0.3648 - val_accuracy: 0.8872 - val_f1_m: 0.8875\n",
      "Epoch 35/150\n",
      "270/270 - 17s - loss: 0.0796 - accuracy: 0.9720 - f1_m: 0.9705 - val_loss: 0.3660 - val_accuracy: 0.8968 - val_f1_m: 0.8965\n",
      "Epoch 36/150\n",
      "270/270 - 17s - loss: 0.0750 - accuracy: 0.9748 - f1_m: 0.9742 - val_loss: 0.3553 - val_accuracy: 0.8982 - val_f1_m: 0.9004\n",
      "Epoch 37/150\n",
      "270/270 - 17s - loss: 0.0742 - accuracy: 0.9732 - f1_m: 0.9732 - val_loss: 0.3624 - val_accuracy: 0.8798 - val_f1_m: 0.8832\n",
      "Epoch 38/150\n",
      "270/270 - 17s - loss: 0.0720 - accuracy: 0.9757 - f1_m: 0.9756 - val_loss: 0.3879 - val_accuracy: 0.8835 - val_f1_m: 0.8851\n",
      "Epoch 39/150\n",
      "270/270 - 17s - loss: 0.0682 - accuracy: 0.9777 - f1_m: 0.9770 - val_loss: 0.4083 - val_accuracy: 0.8827 - val_f1_m: 0.8825\n",
      "Epoch 40/150\n",
      "270/270 - 17s - loss: 0.0671 - accuracy: 0.9767 - f1_m: 0.9756 - val_loss: 0.3378 - val_accuracy: 0.8945 - val_f1_m: 0.8968\n",
      "Epoch 41/150\n",
      "270/270 - 17s - loss: 0.0649 - accuracy: 0.9788 - f1_m: 0.9784 - val_loss: 0.3052 - val_accuracy: 0.9063 - val_f1_m: 0.9110\n",
      "Epoch 42/150\n",
      "270/270 - 17s - loss: 0.0624 - accuracy: 0.9796 - f1_m: 0.9788 - val_loss: 0.3350 - val_accuracy: 0.8975 - val_f1_m: 0.9017\n",
      "Epoch 43/150\n",
      "270/270 - 17s - loss: 0.0604 - accuracy: 0.9810 - f1_m: 0.9805 - val_loss: 0.3519 - val_accuracy: 0.9019 - val_f1_m: 0.9040\n",
      "Epoch 44/150\n",
      "270/270 - 17s - loss: 0.0613 - accuracy: 0.9809 - f1_m: 0.9801 - val_loss: 0.3945 - val_accuracy: 0.8857 - val_f1_m: 0.8874\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 128, 9)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_48 (Bidirectio (None, 128, 64)           10752     \n",
      "_________________________________________________________________\n",
      "bidirectional_49 (Bidirectio (None, 128, 64)           24832     \n",
      "_________________________________________________________________\n",
      "reshape_39 (Reshape)         (None, 128, 64, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 62, 30, 64)        1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 31, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 29, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_24  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 112,652\n",
      "Trainable params: 112,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "270/270 - 629s - loss: 91.5765 - accuracy: 0.2663 - f1_m: 0.0381 - val_loss: 3.6595 - val_accuracy: 0.3282 - val_f1_m: 0.0118\n",
      "Epoch 2/150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24544/3779029576.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel_no\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_func\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     history = model.fit(X_train, Y_train, epochs=150, batch_size=32, validation_data=(X_val, Y_val),verbose=2,callbacks=callbacks(model_name[model_no], patience=30,\n\u001b[0m\u001b[0;32m      7\u001b[0m     location = \"G:\\\\Project Server Pred\\\\4-2 project\\\\model-weights\"))\n\u001b[0;32m      8\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"G:\\\\Project Server Pred\\\\4-2 project\\\\model-weights\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model_{}_norm_{}_h'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_no\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"UCI\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1191\u001b[0m                 _r=1):\n\u001b[0;32m   1192\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_list = [LSTM_CNN_SANDWICH,LSTM_CNN,ConvLSTM_PARALLEL,LSTM_CNN_PARALLEL]\n",
    "model_name = ['LSTM_CNN_SANDWICH','LSTM_CNN','ConvLSTM_PARALLEL','LSTM_CNN_PARALLEL']\n",
    "\n",
    "for model_no,model_func in enumerate(model_list):\n",
    "    model = model_func(X_train.shape[1:],Y_train.shape[1])\n",
    "    history = model.fit(X_train, Y_train, epochs=150, batch_size=32, validation_data=(X_val, Y_val),verbose=2,callbacks=callbacks(model_name[model_no], patience=30,\n",
    "    location = \"G:\\\\Project Server Pred\\\\4-2 project\\\\model-weights\"))\n",
    "    pickle.dump(history.history,open(os.path.join(\"G:\\\\Project Server Pred\\\\4-2 project\\\\model-weights\", 'model_{}_norm_{}_h').format(model_name[model_no], \"UCI\"), 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 128, 9)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 128, 64)           10752     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 128, 64)           24832     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 128, 64, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 62, 30, 64)        1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 31, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 29, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_6 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 112,007\n",
      "Trainable params: 112,007\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "270/270 - 21s - loss: 0.7265 - accuracy: 0.7244 - f1_m: 0.6814 - val_loss: 0.3131 - val_accuracy: 0.9255 - val_f1_m: 0.9193\n",
      "Epoch 2/150\n",
      "270/270 - 9s - loss: 0.2491 - accuracy: 0.9263 - f1_m: 0.9256 - val_loss: 1.0625 - val_accuracy: 0.7478 - val_f1_m: 0.7337\n",
      "Epoch 3/150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14488/1159614090.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM_CNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1191\u001b[0m                 _r=1):\n\u001b[0;32m   1192\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTM_CNN(X_train.shape[1:],Y_train.shape[1])\n",
    "model.fit(X_train, Y_train, epochs=150, batch_size=32, validation_data=(X_val, Y_val),verbose=2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bee25d6519add328f80f5f41ccb25f88a9619954376df032e77982eba47e7184"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
